{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "835a1f29-ee68-4cc4-ac51-f0c0921690f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Variational Autoencoder (VAE) and Hamiltonian Variational Autoencoder (HVAE) implementation.\n",
    "\n",
    "This module contains PyTorch implementations of VAE and HVAE models for unsupervised learning\n",
    "and generative modeling tasks.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as dist\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5ca4f26-e95e-4a45-99da-e1f42d6774cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, args, avg_logit):\n",
    "        super(VAE, self).__init__()\n",
    "        self.z_dim = args.z_dim\n",
    "        self.avg_logit = avg_logit\n",
    "        self._set_net_params()\n",
    "\n",
    "    def get_elbo(self, x, args):\n",
    "        q_mu, q_sigma = self._inf_network(x)\n",
    "        q_z = dist.Normal(q_mu, q_sigma)\n",
    "        \n",
    "        z = q_z.rsample()\n",
    "        p_x_given_z_logits = self._gen_network(z)\n",
    "        p_z = dist.Normal(torch.zeros_like(z), torch.ones_like(z))\n",
    "\n",
    "        kl = dist.kl_divergence(q_z, p_z).sum(1)\n",
    "        expected_log_likelihood = self._bernoulli_log_likelihood(x, p_x_given_z_logits)\n",
    "        elbo = (expected_log_likelihood - kl).mean()\n",
    "\n",
    "        return elbo\n",
    "\n",
    "    def get_nll(self, x, args):\n",
    "        q_mu, q_sigma = self._inf_network(x)\n",
    "        q_z = dist.Normal(q_mu, q_sigma)\n",
    "\n",
    "        z = q_z.rsample()\n",
    "        p_x_given_z_logits = self._gen_network(z)\n",
    "        expected_log_likelihood = self._bernoulli_log_likelihood(x, p_x_given_z_logits)\n",
    "\n",
    "        prior_minus_gen = (-0.5 * (z**2).sum(1) \n",
    "                           + q_sigma.log().sum(1)\n",
    "                           + 0.5 * (((z - q_mu) / q_sigma)**2).sum(1))\n",
    "        nll_samples = expected_log_likelihood + prior_minus_gen\n",
    "\n",
    "        nll_samples_reshaped = nll_samples.view(args.n_IS, args.n_batch_test)\n",
    "        nll_lse = torch.logsumexp(nll_samples_reshaped, dim=0)\n",
    "        nll = np.log(args.n_IS) - nll_lse.mean()\n",
    "\n",
    "        return nll\n",
    "\n",
    "    def get_samples(self, args):\n",
    "        z_0 = torch.randn(args.n_gen_samples, self.z_dim, device=self.fc_mu.weight.device)\n",
    "        logits = self._gen_network(z_0)\n",
    "        samples = torch.sigmoid(logits)\n",
    "        return samples\n",
    "\n",
    "    def _set_net_params(self):\n",
    "        # Inference network\n",
    "        self.conv1 = nn.Conv2d(1, 16, 5, stride=2, padding=2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 5, stride=2, padding=2)\n",
    "        self.conv3 = nn.Conv2d(32, 32, 5, stride=1, padding=2)\n",
    "        self.fc1 = nn.Linear(32 * 7 * 7, 450)\n",
    "        self.fc_mu = nn.Linear(450, self.z_dim)\n",
    "        self.fc_sigma = nn.Linear(450, self.z_dim)\n",
    "\n",
    "        # Generative network\n",
    "        self.fc_g1 = nn.Linear(self.z_dim, 450)\n",
    "        self.fc_g2 = nn.Linear(450, 32 * 7 * 7)\n",
    "        self.deconv1 = nn.ConvTranspose2d(32, 32, 5, stride=1, padding=2)\n",
    "        self.deconv2 = nn.ConvTranspose2d(32, 16, 5, stride=2, padding=2, output_padding=1)\n",
    "        self.deconv3 = nn.ConvTranspose2d(16, 1, 5, stride=2, padding=2, output_padding=1)\n",
    "\n",
    "    def _inf_network(self, x):\n",
    "        h1 = F.softplus(self.conv1(x))\n",
    "        h2 = F.softplus(self.conv2(h1))\n",
    "        h3 = F.softplus(self.conv3(h2))\n",
    "        h3_flat = h3.view(h3.size(0), -1)\n",
    "        h4 = F.softplus(self.fc1(h3_flat))\n",
    "        mu = self.fc_mu(h4)\n",
    "        sigma = F.softplus(self.fc_sigma(h4))\n",
    "        return mu, sigma\n",
    "\n",
    "    def _gen_network(self, z):\n",
    "        h1 = F.softplus(self.fc_g1(z))\n",
    "        h2 = F.softplus(self.fc_g2(h1))\n",
    "        h2_2d = h2.view(h2.size(0), 32, 7, 7)\n",
    "        h3 = F.softplus(self.deconv1(h2_2d))\n",
    "        h4 = F.softplus(self.deconv2(h3))\n",
    "        logits = self.deconv3(h4)\n",
    "        return logits\n",
    "\n",
    "    def _bernoulli_log_likelihood(self, x, logits):\n",
    "        return -F.binary_cross_entropy_with_logits(logits, x, reduction='none').sum([1, 2, 3])\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as dist\n",
    "import numpy as np\n",
    "\n",
    "class HVAE(VAE):\n",
    "    def __init__(self, args, avg_logit):\n",
    "        super().__init__(args, avg_logit)\n",
    "        self.K = args.K\n",
    "        self.args = args  # Store args for later use\n",
    "        self._init_hvae_params()\n",
    "\n",
    "    def _init_hvae_params(self):\n",
    "        init_lf = self.args.init_lf * np.ones(self.z_dim)\n",
    "        init_lf_reparam = np.log(init_lf / (self.args.max_lf - init_lf))\n",
    "\n",
    "        if self.args.vary_eps == 'true':\n",
    "            init_lf_reparam = np.tile(init_lf_reparam, (self.K, 1))\n",
    "\n",
    "        self.lf_reparam = nn.Parameter(torch.tensor(init_lf_reparam, dtype=torch.float32))\n",
    "        self.temp_method = self.args.temp_method\n",
    "\n",
    "        if self.temp_method == 'free':\n",
    "            init_alphas = self.args.init_alpha * np.ones(self.K)\n",
    "            init_alphas_reparam = np.log(init_alphas / (1 - init_alphas))\n",
    "            self.alphas_reparam = nn.Parameter(torch.tensor(init_alphas_reparam, dtype=torch.float32))\n",
    "        elif self.temp_method == 'fixed':\n",
    "            init_T_0 = self.args.init_T_0\n",
    "            init_T_0_reparam = np.log(init_T_0 - 1)\n",
    "            self.T_0_reparam = nn.Parameter(torch.tensor(init_T_0_reparam, dtype=torch.float32))\n",
    "        elif self.temp_method == 'none':\n",
    "            self.register_buffer('T_0', torch.tensor(1., dtype=torch.float32))\n",
    "            self.register_buffer('alphas', torch.ones(self.K, dtype=torch.float32))\n",
    "        else:\n",
    "            raise ValueError(f'Tempering method {self.temp_method} not supported')\n",
    "\n",
    "    def get_elbo(self, x, args):\n",
    "        q_mu, q_sigma = self._inf_network(x)\n",
    "\n",
    "        z_0 = q_mu + q_sigma * torch.randn_like(q_mu)\n",
    "        p_0 = torch.sqrt(self.T_0) * torch.randn_like(q_mu)\n",
    "\n",
    "        z_K, p_K = self._his(z_0, p_0, x, args)\n",
    "\n",
    "        p_x_given_zK_logits = self._gen_network(z_K)\n",
    "        expected_log_likelihood = self._bernoulli_log_likelihood(x, p_x_given_zK_logits)\n",
    "\n",
    "        log_prob_zK = -0.5 * (z_K**2).sum(1)\n",
    "        log_prob_pK = -0.5 * (p_K**2).sum(1)\n",
    "        sum_log_sigma = q_sigma.log().sum(1)\n",
    "\n",
    "        neg_kl_term = log_prob_zK + log_prob_pK + sum_log_sigma + self.z_dim\n",
    "        elbo = (expected_log_likelihood + neg_kl_term).mean()\n",
    "\n",
    "        return elbo\n",
    "\n",
    "    def get_nll(self, x, args):\n",
    "        q_mu, q_sigma = self._inf_network(x)\n",
    "\n",
    "        z_0 = q_mu + q_sigma * torch.randn_like(q_mu)\n",
    "        p_0 = torch.sqrt(self.T_0) * torch.randn_like(q_mu)\n",
    "\n",
    "        z_K, p_K = self._his(z_0, p_0, x, args)\n",
    "\n",
    "        p_x_given_zK_logits = self._gen_network(z_K)\n",
    "        expected_log_likelihood = self._bernoulli_log_likelihood(x, p_x_given_zK_logits)\n",
    "\n",
    "        log_prob_zK = -0.5 * (z_K**2).sum(1)\n",
    "        log_prob_pK = -0.5 * (p_K**2).sum(1)\n",
    "        sum_log_sigma = q_sigma.log().sum(1)\n",
    "\n",
    "        log_prob_z0 = -0.5 * (((z_0 - q_mu) / q_sigma)**2).sum(1)\n",
    "        log_prob_p0 = -0.5 / self.T_0 * (p_0**2).sum(1)\n",
    "\n",
    "        nll_samples = (expected_log_likelihood + log_prob_zK + log_prob_pK\n",
    "                       + sum_log_sigma - log_prob_z0 - log_prob_p0)\n",
    "\n",
    "        nll_samples_reshaped = nll_samples.view(args.n_IS, args.n_batch_test)\n",
    "        nll_lse = torch.logsumexp(nll_samples_reshaped, dim=0)\n",
    "        nll = np.log(args.n_IS) - nll_lse.mean()\n",
    "\n",
    "        return nll\n",
    "\n",
    "    def _his(self, z_0, p_0, x, args):\n",
    "        z = z_0\n",
    "        p = p_0\n",
    "\n",
    "        for k in range(1, self.K + 1):\n",
    "            if args.vary_eps == 'true':\n",
    "                lf_eps = self.lf_eps[k-1, :]\n",
    "            else:\n",
    "                lf_eps = self.lf_eps\n",
    "\n",
    "            p_half = p - 0.5 * lf_eps * self._dU_dz(z, x)\n",
    "            z = z + lf_eps * p_half\n",
    "            p_temp = p_half - 0.5 * lf_eps * self._dU_dz(z, x)\n",
    "\n",
    "            p = self.alphas[k-1] * p_temp\n",
    "\n",
    "        return z, p\n",
    "\n",
    "    def _dU_dz(self, z, x):\n",
    "        z.requires_grad_(True)\n",
    "        net_out = self._gen_network(z)\n",
    "        U = F.softplus(net_out).sum((1, 2, 3)) - (x * net_out).sum((1, 2, 3))\n",
    "        grad_U = torch.autograd.grad(U.sum(), z)[0] + z\n",
    "        return grad_U\n",
    "\n",
    "    @property\n",
    "    def lf_eps(self):\n",
    "        return torch.sigmoid(self.lf_reparam) * self.args.max_lf\n",
    "\n",
    "    @property\n",
    "    def alphas(self):\n",
    "        if self.temp_method == 'free':\n",
    "            return torch.sigmoid(self.alphas_reparam)\n",
    "        elif self.temp_method == 'fixed':\n",
    "            T_0 = 1 + torch.exp(self.T_0_reparam)\n",
    "            k_vec = torch.arange(1, self.K + 1, dtype=torch.float32, device=self.lf_reparam.device)\n",
    "            k_m_1_vec = torch.arange(0, self.K, dtype=torch.float32, device=self.lf_reparam.device)\n",
    "            temp_sched = (1 - T_0) * k_vec**2 / self.K**2 + T_0\n",
    "            temp_sched_m_1 = (1 - T_0) * k_m_1_vec**2 / self.K**2 + T_0\n",
    "            return torch.sqrt(temp_sched / temp_sched_m_1)\n",
    "        else:\n",
    "            return self.alphas\n",
    "\n",
    "    @property\n",
    "    def T_0(self):\n",
    "        if self.temp_method == 'free':\n",
    "            return torch.prod(self.alphas)**(-2)\n",
    "        elif self.temp_method == 'fixed':\n",
    "            return 1 + torch.exp(self.T_0_reparam)\n",
    "        else:\n",
    "            return self.T_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b89b0ed2-cc24-4e8c-a6a5-e93cf4105118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edlu/opt/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:266: UserWarning: MPS: nonzero op is supported natively starting from macOS 13.0. Falling back on CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/Indexing.mm:283.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 650.498840\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 205.968430\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/18/wz5v3z6x7_gdgjx4t9zvjzxc0000gn/T/ipykernel_14188/802653327.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     83\u001B[0m \u001B[0;31m# Main training loop\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     84\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mepoch\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mEPOCHS\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 85\u001B[0;31m     \u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mepoch\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     86\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mepoch\u001B[0m \u001B[0;34m%\u001B[0m \u001B[0;36m10\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     87\u001B[0m         \u001B[0mvisualize_reconstructions\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mepoch\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/var/folders/18/wz5v3z6x7_gdgjx4t9zvjzxc0000gn/T/ipykernel_14188/802653327.py\u001B[0m in \u001B[0;36mtrain\u001B[0;34m(epoch)\u001B[0m\n\u001B[1;32m     48\u001B[0m             \u001B[0melbo\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_elbo\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     49\u001B[0m             \u001B[0mloss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m-\u001B[0m\u001B[0melbo\u001B[0m  \u001B[0;31m# Negative ELBO is the loss we want to minimize\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 50\u001B[0;31m             \u001B[0mloss\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     51\u001B[0m             \u001B[0mtrain_loss\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mloss\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mitem\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     52\u001B[0m             \u001B[0moptimizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/_tensor.py\u001B[0m in \u001B[0;36mbackward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    520\u001B[0m                 \u001B[0minputs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    521\u001B[0m             )\n\u001B[0;32m--> 522\u001B[0;31m         torch.autograd.backward(\n\u001B[0m\u001B[1;32m    523\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgradient\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    524\u001B[0m         )\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py\u001B[0m in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    264\u001B[0m     \u001B[0;31m# some Python versions print out the first line of a multi-line function\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    265\u001B[0m     \u001B[0;31m# calls in the traceback and some print out the last line\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 266\u001B[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001B[0m\u001B[1;32m    267\u001B[0m         \u001B[0mtensors\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    268\u001B[0m         \u001B[0mgrad_tensors_\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "#from vae import VAE  # Assuming the VAE class is in a file named vae.py\n",
    "\n",
    "# Hyperparameters and settings\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 1e-3\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Data loading\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Model setup\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.z_dim = 20\n",
    "        self.n_IS = 1  # For NLL calculation\n",
    "        self.n_batch_test = BATCH_SIZE\n",
    "        self.n_gen_samples = 16  # For generating samples\n",
    "        self.K = 5  # Number of leapfrog steps\n",
    "        self.init_lf = 0.01  # Initial leapfrog step size\n",
    "        self.max_lf = 0.1  # Maximum leapfrog step size\n",
    "        self.vary_eps = 'false'  # Whether to vary epsilon across layers\n",
    "        self.temp_method = 'free'  # Tempering method: 'free', 'fixed', or 'none'\n",
    "        self.init_alpha = 0.9  # Initial alpha for free tempering\n",
    "        self.init_T_0 = 2.0  # Initial temperature for fixed tempering\n",
    "\n",
    "args = Args()\n",
    "avg_logit = 0.0  # You may want to calculate this based on your data\n",
    "model = HVAE(args, avg_logit).to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Training loop\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        try:\n",
    "            elbo = model.get_elbo(data, args)\n",
    "            loss = -elbo  # Negative ELBO is the loss we want to minimize\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            optimizer.step()\n",
    "        \n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n",
    "                      f'({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "        except Exception as e:\n",
    "            print(f\"Error in batch {batch_idx}: {str(e)}\")\n",
    "            print(f\"Input shape: {data.shape}\")\n",
    "            print(f\"Input min: {data.min()}, max: {data.max()}\")\n",
    "            raise\n",
    "    \n",
    "    print(f'====> Epoch: {epoch} Average loss: {train_loss / len(train_loader):.4f}')\n",
    "\n",
    "# Visualization function\n",
    "def visualize_reconstructions(epoch):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        sample = next(iter(train_loader))[0][:8].to(DEVICE)\n",
    "        recon = model._gen_network(model._inf_network(sample)[0])\n",
    "        recon = torch.sigmoid(recon)\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 8, figsize=(16, 4))\n",
    "        for i in range(8):\n",
    "            axes[0, i].imshow(sample[i].cpu().squeeze(), cmap='gray')\n",
    "            axes[0, i].axis('off')\n",
    "            axes[1, i].imshow(recon[i].cpu().squeeze(), cmap='gray')\n",
    "            axes[1, i].axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'reconstruction_epoch_{epoch}.png')\n",
    "        plt.close()\n",
    "\n",
    "# Main training loop\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(epoch)\n",
    "    if epoch % 10 == 0:\n",
    "        visualize_reconstructions(epoch)\n",
    "\n",
    "# Generate samples after training\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    sample = model.get_samples(args)\n",
    "    fig, axes = plt.subplots(4, 4, figsize=(8, 8))\n",
    "    for i in range(16):\n",
    "        ax = axes[i//4, i%4]\n",
    "        ax.imshow(sample[i].cpu().squeeze(), cmap='gray')\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('generated_samples.png')\n",
    "    plt.close()\n",
    "\n",
    "print(\"Training complete. Reconstructions and generated samples have been saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5094914-220e-4382-b695-819e609b2d08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}